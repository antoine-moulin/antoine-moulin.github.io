[{"authors":null,"categories":null,"content":"I am a Ph.D. student in reinforcement learning for combinatorial optimization at Inria/CNRS in the SequeL/ScooL team, under the supervision of P. Preux. My research interests also include graph representation learning and geometric deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nathan-grinsztajn.netlify.app/author/nathan-grinsztajn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nathan-grinsztajn/","section":"authors","summary":"I am a Ph.D. student in reinforcement learning for combinatorial optimization at Inria/CNRS in the SequeL/ScooL team, under the supervision of P. Preux. My research interests also include graph representation learning and geometric deep learning.","tags":null,"title":"Nathan Grinsztajn","type":"authors"},{"authors":null,"categories":null,"content":"Supervisors: Philippe Preux, Nathan Grinsztajn\nDuration: 5 to 6 months\nWhen: Spring-Summer 2021\nWhere: Scool (previously SequeL), Inria Lille, Villeneuve d\u0026rsquo;Ascq, France\nExpected background: master in CS, specialized in machine learning.\nKeywords: reinforcement learning, combinatorial optimization, experimental.\nContext: Reinforcement learning is a sub-field of machine learning in which we aim at designing agents that learn to act. Acting usually involves performing a sequence of actions in order to achieve a goal. Examples are countless; games are good examples, like Pacman or chess in which the player has to perform a series of actions either to reach a maximal score, or to defeat his opponent. Applications of RL go way beyond games.\nRecently, there were trials to use RL to solve hard combinatorial problems. Some major examples (and successes) are GO, Rubiks Cube, Scheduling, Maximum Independent Set (MIS), Maximum Coverage (MC), Minimum Vertex Cover (MVC)\u0026hellip; Graph coloring, however useful in practice (bandwidth allocation, scheduling\u0026hellip;) and difficult, received less attention. State of the art graph coloring solvers still mainly rely on tabu search (tabucol), and hand-crafted heuristics. It is to be expected that hybriding RL and tabu search will provide more efficient algorithms.\nWhat: The goal of this internship is to:\n Study the literature of this problem. This includes deep reinforcement learning, graph neural networks, graph coloring, tabu search. Perform an experimental assessment of the ideas. Explore new ideas on combining RL and tabu search (or other search algorithms). This exploration can be theoretical or algorithmic.  Bibliography:\n Sutton, Barto, Reinforcement Learning, an Introduction, 2nd edition, 2018. Battaglia et al. Relational Inductive Biases, Deep Learning, and Graph Networks ArXiv:1806.01261, October 17, 2018. A. Hertz, D. de Werra, Using tabu search techniques for graph coloring. Computing 39, 345â€“351 (1987).  Working environment: Scool (previously SequeL) is a well-known research group in reinforcement learning and bandits. It is composed of 5 permanent researchers, 20+ PhD students, a couple of post-docs and engineers. Scool provides a very rich and stimulating for doing cutting-edge research in RL.\n","date":1605826800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605826800,"objectID":"6732ebb52b9059fd605dff6edf50ba4b","permalink":"https://nathan-grinsztajn.netlify.app/internships/tabucol/","publishdate":"2020-11-20T00:00:00+01:00","relpermalink":"/internships/tabucol/","section":"internships","summary":"Learning to solve graph coloring problems and exploring ideas combining RL and tabu search for combinatorial optimization.","tags":null,"title":"RL and Tabu Search for Graph Coloring Problems","type":"internships"},{"authors":null,"categories":null,"content":"Supervisors: Philippe Preux, Nathan Grinsztajn\nDuration: 5 to 6 months\nWhen: Spring-Summer 2021\nWhere: Scool (previously SequeL), Inria Lille, Villeneuve d\u0026rsquo;Ascq, France\nExpected background: master in CS, specialized in machine learning.\nKeywords: reinforcement learning, combinatorial optimization, experimental.\nContext: Combinatorial Optimization Problems (COP) constitute an important family of fundamental problems: traveling salesman, vehicle routing problem, stable marriage , graph coloring, task scheduling, and many others. There are various algorithmic approaches, ranging from (provably) exact methods (e.g. based on tree search, linear programming\u0026hellip;) to non (provably) exact/approximate methods (heuristics and meta-heuristics). Those methods are able to solve large scale COPs, but they require a careful investigation of the problem.\nOn the other hand, real world applications bring another set of challenges: inherent uncertainty in the definition of the problem and randomness in the process dynamics. Task Graph Scheduling consists in mapping a task graph onto a target platform: nodes denote computational tasks, and edges model precedence constraints between tasks. When the full graph is unknown at the beginning of the scheduling (e.g. because of user interactions), known heuristics fall short. We propose to study and implement a RL approach to solve this kind of task.\nWhat: The goal of this internship is to:\n Study the literature of this problem. This includes deep reinforcement learning, graph neural networks, task graph scheduling. Perform an experimental assessment of these ideas. Explore new ideas on combining RL and dynamic graphs (e.g. the Canadian Traveller Problem (CTP)). This exploration can be theoretical or algorithmic.  Bibliography:\n Sutton, Barto, Reinforcement Learning, an Introduction, 2nd edition, 2018 Aditya Paliwal et al, Reinforced genetic algorithm learning for optimizing computation graphs. InProc. ICLR, page 24,2020 Battaglia et al. Relational Inductive Biases, Deep Learning, and Graph Networks ArXiv:1806.01261, October 17, 2018. Grinsztajn et al, Geometric Deep Reinforcement Learning for Dynamic DAG Scheduling. Advances in IEEE ADPRL (ADPRL 2020).  Working environment: Scool (previously SequeL) is a well-known research group in reinforcement learning and bandits. It is composed of 5 permanent researchers, 20+ PhD students, a couple of post-docs and engineers. Scool provides a very rich and stimulating for doing cutting-edge research in RL.\n","date":1605826800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605826800,"objectID":"b0a0e9e0b13fe3ff6b6182c9f857dc14","permalink":"https://nathan-grinsztajn.netlify.app/internships/hpc/","publishdate":"2020-11-20T00:00:00+01:00","relpermalink":"/internships/hpc/","section":"internships","summary":"Task Graph Scheduling is the activity that consists in mapping a task graph onto a target platform: nodes denote computational tasks, and edges model precedence constraints between tasks. When the full graph is unknown at the beginning of the scheduling (e.g. because of user interactions), known heuristics fall short. We propose to study and implement a RL approach to solve this kind of task.","tags":null,"title":"RL for Dynamic Task Graph Scheduling","type":"internships"},{"authors":["Nathan Grinsztajn","Olivier Beaumont","Emmanuel Jeannot","Philippe Preux"],"categories":null,"content":"","date":1603365120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603365120,"objectID":"9fbf800c5db8fbe802a28720ab54f213","permalink":"https://nathan-grinsztajn.netlify.app/publication/hpcadprl/","publishdate":"2020-10-22T12:12:00+01:00","relpermalink":"/publication/hpcadprl/","section":"publication","summary":"In practice, it is quite common to face combinatorial optimization problems which contain uncertainty along with non-determinism and dynamicity. These three properties call for appropriate algorithms; reinforcement learning (RL) is dealing with them in a very natural way. Today, despite some efforts, most real-life combinatorial optimization problems remain out of the reach of reinforcement learning algorithms. In this paper, we propose a reinforcement learning approach to solve a realistic scheduling problem, and apply it to an algorithm commonly executed in the high performance computing community, the Cholesky factorization. On the contrary to static scheduling, where tasks are assigned to processors in a predetermined ordering before the beginning of the parallel execution, our method is dynamic: task allocations and their execution ordering are decided at runtime, based on the system state and unexpected events, which allows much more flexibility. To do so, our algorithm uses graph neural networks in combination with an actor-critic algorithm (A2C) to build an adaptive representation of the problem on the fly. We show that this approach is competitive with state-of-the-art heuristics used in high-performance computing runtime systems. Moreover, our algorithm does not require an explicit model of the environment, but we demonstrate that extra knowledge can easily be incorporated and improves performance. We also exhibit key properties provided by this RL approach, and study its transfer abilities to other instances.","tags":[],"title":"Geometric Deep Reinforcement Learning for Dynamic DAG Scheduling","type":"publication"}]